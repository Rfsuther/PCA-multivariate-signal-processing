{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE: PCA and multivariate neural signal processing\n",
    "## SECTION: Linear algebra crash course\n",
    "### VIDEO: Python: Eigendecomposition and SVD (SOLUTIONS file)\n",
    "#### Instructor: sincxpress.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Eigendecomposition\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This cell shows a geometric picture of eigenvectors in 2D.\n",
    "#  Notice that the two red lines (v and Av) are collinear, \n",
    "#  but not the two black lines (w and Aw). v is an eigenvector, w is not.\n",
    "\n",
    "\n",
    "\n",
    "# matrix\n",
    "A = [ [1,5], [2,4] ]\n",
    "\n",
    "# extract the eigenvalues\n",
    "eigvals = np.linalg.eig(A)\n",
    "\n",
    "# note that the eigenvalues are in the first element of eigvals:\n",
    "print(eigvals[0])\n",
    "\n",
    "\n",
    "# specify two vectors\n",
    "v1 = np.array([ 1,1 ])    # is an eigenvector!\n",
    "v2 = np.random.randn(2,1) # unlikely to be an eigenvector\n",
    "v2 = v2/np.linalg.norm(v2)# unit length for convenience\n",
    "\n",
    "# compute Av\n",
    "Av1 = A@v1\n",
    "Av2 = A@v2\n",
    "\n",
    "\n",
    "# plot the vectors and Av\n",
    "plt.plot([0,v1[0]] ,[0,v1[1]],'r')\n",
    "plt.plot([0,Av1[0]],[0,Av1[1]],'r--')\n",
    "plt.plot([0,v2[0]] ,[0,v2[1]],'k')\n",
    "plt.plot([0,Av2[0]],[0,Av2[1]],'k--')\n",
    "\n",
    "plt.axis([-8,8,-8,8])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# QUESTION: Is there something special about vector v? To find out,\n",
    "#           change one value of matrix A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### eigenvectors of symmetric matrices\n",
    "\n",
    "# create a random matrix\n",
    "A = np.random.randn(14,14)\n",
    "\n",
    "# make it symmetric\n",
    "S = A@A.T\n",
    "\n",
    "# diagonalize it\n",
    "evals,evecs = np.linalg.eig(S)\n",
    "\n",
    "\n",
    "# magnitudes of each vector\n",
    "print( np.sqrt( sum(evecs**2) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and make plots\n",
    "plt.imshow(S)\n",
    "plt.axis('off')\n",
    "plt.title('S')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(evecs)\n",
    "plt.axis('off')\n",
    "plt.title('Eigenvectors')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(evecs.T@evecs)\n",
    "plt.axis('off')\n",
    "plt.title('V$^T$V')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#   QUESTION: If VtV = I, then Vt = V^-1. How can you verify this in Python?\n",
    "# \n",
    "# Answer: you can inspect evecs' - inv(evecs). Note that due to computer\n",
    "# rounding errors, 1e-15 (10^-15) can be considered zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# side note about the eig() function\n",
    "\n",
    "# numpy's eig output is different from MATLAB's eig output\n",
    "# numpy returns a tuple with the first element being a vector of eigenvalues, \n",
    "# and the second element the matrix of eigenvectors\n",
    "lambdas = np.linalg.eig(S)\n",
    "print(len(lambdas))\n",
    "print(lambdas)\n",
    "\n",
    "# To diagonalize:\n",
    "eigvals,eigvecs = np.linalg.eig(S)\n",
    "print(' ')\n",
    "np.diag(eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fun with random matrices\n",
    "\n",
    "# Random matrices tend to have complex eigenvalues\n",
    "# Those eigenvalues form a circle in the complex plane.\n",
    "\n",
    "nIter = 150\n",
    "matsize = 40\n",
    "evals = np.zeros((nIter,matsize),dtype=np.complex)\n",
    "\n",
    "for i in range(nIter):\n",
    "    X = np.random.randn(matsize,matsize)\n",
    "    evals[i,:] = np.linalg.eig(X)[0]\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(np.real(evals),np.imag(evals),'ro',markerfacecolor='k')\n",
    "plt.xlabel('Real part'), plt.ylabel('Imaginary part')\n",
    "plt.axis('square')\n",
    "plt.show()\n",
    "\n",
    "# Note: This is just a fun FYI; no practical applications for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Singular value decomposition (SVD)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will show that eigendecomposition and SVD are the same thing for\n",
    "# symmetric matrices (they are *not* the same for other kinds of matrices;\n",
    "# this is an example of a special property of symmetric matrices).\n",
    "\n",
    "# create a symmetric matrix\n",
    "X = np.random.randn(5,5)\n",
    "S = X.T@X\n",
    "\n",
    "# take its eigendecomposition\n",
    "L_eig,W_eig = np.linalg.eig(S)\n",
    "\n",
    "# take its SVD\n",
    "U_svd,S_svd,V_svd = np.linalg.svd(S)\n",
    "\n",
    "\n",
    "# If they are identical, their difference should be zero:\n",
    "np.round( W_eig - U_svd ,4)\n",
    "\n",
    "# deffo not zero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.vstack( (L_eig,S_svd) ).T)\n",
    "# ah, they need to be sorted ;)\n",
    "\n",
    "sidx = np.argsort(L_eig)[::-1]\n",
    "\n",
    "# now try again:\n",
    "print(' ')\n",
    "print( np.vstack( (L_eig[sidx],S_svd) ).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try the vectors\n",
    "print(np.round( W_eig[:,sidx] - U_svd ,4))\n",
    "\n",
    "# I guess some columns are zero and others not? The issue now is the sign\n",
    "# uncertainty of eigenvectors:\n",
    "print(' ')\n",
    "print(np.round( (-W_eig[:,sidx]) - U_svd ,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SVD of Einstein\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import picture\n",
    "pic = Image.open('einstein.jpg')\n",
    "\n",
    "# let's have a look\n",
    "plt.imshow(pic)\n",
    "plt.show()\n",
    "\n",
    "# we need to convert it to 2D floating-point precision\n",
    "pic = np.array(pic)\n",
    "pic = np.mean(pic,axis=2)\n",
    "plt.imshow(pic,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD (singular value decomposition)\n",
    "\n",
    "U,S,V = np.linalg.svd( pic )\n",
    "\n",
    "# compute percent-variance explained sigmas\n",
    "singvalspct = 100*S/sum(S)\n",
    "\n",
    "\n",
    "_,axs = plt.subplots(1,3,figsize=(6,4))\n",
    "\n",
    "# image the three matrices\n",
    "axs[0].imshow(U)\n",
    "axs[0].set_title('U (left singular vectors)')\n",
    "\n",
    "axs[1].imshow(np.diag(S),vmin=0,vmax=200)\n",
    "axs[1].set_title('$\\Sigma$ (singular values)')\n",
    "\n",
    "axs[2].imshow(V)\n",
    "axs[2].set_title('V$^T$ (right singular vectors)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the spectrum\n",
    "_,axs = plt.subplots(1,2)\n",
    "axs[0].plot(S,'s-')\n",
    "axs[0].set_xlim([-2,100])\n",
    "axs[0].set_xlabel('Component number')\n",
    "axs[0].set_ylabel('Singular value ($\\sigma$)')\n",
    "\n",
    "axs[1].plot(singvalspct,'s-')\n",
    "axs[1].set_xlim([-2,100])\n",
    "axs[1].set_xlabel('Component number')\n",
    "axs[1].set_ylabel('Singular value (% variance)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the image based on some components\n",
    "\n",
    "#  The goal here is to recreate Einstein using a small number of the most\n",
    "#   important feature dimensions.\n",
    "\n",
    "fig,axs = plt.subplots(4,4,figsize=(7,10))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    \n",
    "    # reconstruct the low-rank version of the picture\n",
    "    lowapp = U[:,:i]@np.diag(S[:i])@V[:i,:]\n",
    "    \n",
    "    # compute its rank\n",
    "    matrix_rank = np.linalg.matrix_rank(lowapp)\n",
    "    \n",
    "    # and visualize!\n",
    "    ax.imshow(lowapp,cmap='gray')\n",
    "    title = f'r={matrix_rank}\\n{np.sum(singvalspct[:i]):.2f}% var.'\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QUESTION: How many components / what percent variance do you need to\n",
    "#           get a \"good\" (subjective) reconstruction of Einstein? \n",
    "#           Does this give you any hope for post-mortem resurrection??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
